{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((sum([len(files) for r, d, files in os.walk(os.getcwd())]), 224, 224, 3))\n",
    "Y = np.zeros((sum([len(files) for r, d, files in os.walk(os.getcwd())]), 7))\n",
    "count = 0\n",
    "for i in range(1, 8):\n",
    "    for file in os.listdir(os.getcwd()+\"/\"+str(i)+\"/\"):\n",
    "        X[count] = cv2.resize(cv2.imread(os.getcwd()+\"/\"+str(i)+\"/\"+file), dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        Y[count][i-1] = 1\n",
    "        count += 1\n",
    "X = X[:count]\n",
    "Y = Y[:count]\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(learning_rate, epochs, batch_size, xTrain, yTrain, xTest, yTest):\n",
    "    x = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]])\n",
    "    y = tf.placeholder(tf.float32, [None, Y.shape[1]])\n",
    "    first_layer = tf.nn.relu(tf.nn.conv2d(x , tf.Variable(tf.truncated_normal([5, 5, X.shape[3], 16], stddev = 0.05), name='W1'), [1, 1, 1, 1], padding='SAME') + tf.Variable(tf.truncated_normal([16]), name='b1'))\n",
    "    pooled_first_layer = tf.nn.max_pool(first_layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1] , padding='SAME')\n",
    "    second_layer = tf.nn.relu(tf.nn.conv2d(pooled_first_layer , tf.Variable(tf.truncated_normal([5, 5, 16, 32], stddev = 0.05), name='W2'), [1, 1, 1, 1], padding='SAME') + tf.Variable(tf.truncated_normal([32]), name='b2'))\n",
    "    pooled_second_layer = tf.nn.max_pool(second_layer, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')\n",
    "    third_layer = tf.nn.relu(tf.nn.conv2d(pooled_second_layer , tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev = 0.05), name='W3'), [1, 1, 1, 1], padding='SAME') + tf.Variable(tf.truncated_normal([64]), name='b3'))\n",
    "    pooled_third_layer = tf.nn.max_pool(third_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    flat_layer = tf.reshape(pooled_third_layer, [-1, 7*7*64])\n",
    "    output = tf.nn.softmax(tf.matmul(flat_layer, tf.Variable(tf.truncated_normal([7*7*64, 7], stddev = 0.05), name='W4')) + tf.Variable(tf.truncated_normal([7]), name='b4'))        \n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels = y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy_loss)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(output, 1)), tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        total_batch = xTrain.shape[0]//batch_size\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            for i in range(total_batch):\n",
    "                _, c = sess.run([optimizer, cross_entropy_loss], feed_dict={x:xTrain[i*batch_size:(i+1)*batch_size], y:yTrain[i*batch_size:(i+1)*batch_size]})\n",
    "                avg_cost += c/total_batch\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x:xTest, y:yTest})\n",
    "            print(\"Epoch:\", (epoch+1))\n",
    "            print(\"Cost:\", avg_cost)\n",
    "            print(\"Test accuracy:\", test_accuracy)\n",
    "        print(\"Training Done\")\n",
    "        print(\"Accuracy obtained:\", sess.run(accuracy, feed_dict = {x:xTest, y:yTest})*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/subhadeep/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch: 1\n",
      "Cost: 1.9655274391174316\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 2\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 3\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 4\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 5\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 6\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 7\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 8\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 9\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Epoch: 10\n",
      "Cost: 1.9654222011566163\n",
      "Test accuracy: 0.21637426\n",
      "Training Done\n",
      "Accuracy obtained: 21.63742631673813\n"
     ]
    }
   ],
   "source": [
    "cnn(0.01, 10, 64, xTrain, yTrain, xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
